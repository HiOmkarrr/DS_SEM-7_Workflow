# -*- coding: utf-8 -*-
"""DS_EXP_5_REGRESSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-J5-79xjoLPNTOGttOwif10nFeAIENAT

# Experiment 5: Explainable AI (SHAP, LIME) & Fairness Audit - REGRESSION VERSION

**Aim:** Provide global and local explanations for model predictions and evaluate fairness across a chosen sensitive attribute using **continuous engagement values** instead of binary classification.

**Objectives**
1. Global explanations: Which features most drive continuous engagement predictions? (SHAP summary & dependence)
2. Local explanations: Why did the model predict this specific engagement score? (LIME)
3. Fairness audit: Are predictive outcomes different across groups for continuous values? (Custom regression fairness metrics)
4. Mitigation: Demonstrate bias correction strategies for regression models.

**Scientific Framing (What / Why / How)**
- **What:** Train regression models to predict continuous product engagement scores from product metadata; then interpret and audit them.
- **Why:** Preserve all information in continuous engagement values while maintaining model transparency and fairness requirements.
- **How:**
  - Use original `product_engagement` as continuous target (no thresholding).
  - Engineer synthetic *sensitive attribute* `speed_bucket` from `delivery_time` (Fast<=3 days, Standard=4-5, Slow>5).
  - Train Random Forest Regressor, Linear Regression, and Gradient Boosting Regressor.
  - Use SHAP TreeExplainer for global feature importance; LIME for local explanations.
  - Develop custom fairness metrics for regression: mean predictions, RMSE, and bias across groups.
  - Apply bias correction through residual adjustment or group-specific calibration.

**Pipeline Overview**
Data → Feature Engineering → Train/Test Split → Regression Models → SHAP (global) → LIME (local) → Regression Fairness Metrics → Bias Mitigation → Conclusions.

---
"""

import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Modeling
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score)

# Explainability
import shap

# LIME (ensure installed: pip install lime)
try:
    from lime.lime_tabular import LimeTabularExplainer
    LIME_AVAILABLE = True
except Exception:
    LIME_AVAILABLE = False
    print("LIME not available (install with: pip install lime). Skipping local explanations.")

# Additional utilities
import warnings
warnings.filterwarnings('ignore')

RANDOM_STATE = 42
DATA_PATH = Path('../../datasets/DS-2-8-25/processed_zudio_data.csv').resolve()

raw = pd.read_csv(DATA_PATH)
print(f"Loaded shape: {raw.shape}")
print("Continuous engagement stats:")
print(raw['product_engagement'].describe())
raw.head()

# === Data Preprocessing ===

# Basic cleaning / type adjustments
# Convert booleans
for col in ['is_bestseller','trending']:
    if raw[col].dtype == object:
        raw[col] = raw[col].astype(str).str.lower().eq('true')

# Target: Keep continuous engagement values
target = 'product_engagement'
print(f"Target variable '{target}' distribution:")
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.hist(raw[target], bins=50, alpha=0.7, edgecolor='black')
plt.title('Product Engagement Distribution')
plt.xlabel('Engagement Score')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
plt.boxplot(raw[target])
plt.title('Product Engagement Box Plot')
plt.ylabel('Engagement Score')
plt.tight_layout()
plt.show()

# Sensitive attribute: derive speed_bucket from delivery_time (string like '4 days')
# Extract numeric days
raw['delivery_days'] = raw['delivery_time'].str.extract(r'(\d+)').astype(int)
raw['speed_bucket'] = pd.cut(raw['delivery_days'], bins=[-np.inf,3,5,np.inf], labels=['Fast','Standard','Slow'])
print("\nDelivery speed distribution:")
print(raw['speed_bucket'].value_counts())

# Analyze engagement by speed bucket
print("\nEngagement by delivery speed:")
engagement_by_speed = raw.groupby('speed_bucket')[target].agg(['mean', 'std', 'count'])
print(engagement_by_speed)

plt.figure(figsize=(10, 6))
sns.boxplot(data=raw, x='speed_bucket', y=target)
plt.title('Product Engagement by Delivery Speed')
plt.ylabel('Engagement Score')
plt.show()

# === Feature Engineering & Model Training ===

# Feature selection
num_features = ['price','original_price','rating','rating_count','discount_percentage','delivery_days']
cat_features = ['category','size','color','material','availability','speed_bucket']
bool_features = ['is_bestseller','trending']

X = raw[num_features + cat_features + bool_features]
y = raw[target]  # Continuous target!

print(f"Feature matrix shape: {X.shape}")
print(f"Target vector shape: {y.shape}")
print(f"Target range: {y.min():.2f} to {y.max():.2f}")

# Preprocessing pipeline
numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
boolean_transformer = 'passthrough'

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_features),
        ('cat', categorical_transformer, cat_features),
        ('bool', boolean_transformer, bool_features)
    ]
)

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=RANDOM_STATE
)
print(f"\nTrain/Test sizes: {X_train.shape}, {X_test.shape}")
print(f"Train target stats: mean={y_train.mean():.2f}, std={y_train.std():.2f}")
print(f"Test target stats: mean={y_test.mean():.2f}, std={y_test.std():.2f}")

# === Regression Models ===

# Define regression models
models = {
    'Linear Regression': Pipeline([
        ('prep', preprocessor),
        ('reg', LinearRegression())
    ]),

    'Random Forest': Pipeline([
        ('prep', preprocessor),
        ('reg', RandomForestRegressor(
            n_estimators=300,
            max_depth=None,
            min_samples_leaf=2,
            n_jobs=-1,
            random_state=RANDOM_STATE
        ))
    ]),

    'Gradient Boosting': Pipeline([
        ('prep', preprocessor),
        ('reg', GradientBoostingRegressor(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=6,
            random_state=RANDOM_STATE
        ))
    ])
}

# Train models and evaluate
trained_models = {}
model_results = {}

for name, model in models.items():
    print(f"\n=== Training {name} ===")

    # Train
    model.fit(X_train, y_train)

    # Predict
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    # Evaluate
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    test_mae = mean_absolute_error(y_test, y_pred_test)

    results = {
        'train_r2': train_r2,
        'test_r2': test_r2,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'predictions': y_pred_test
    }

    model_results[name] = results
    trained_models[name] = model

    print(f"Train R²: {train_r2:.4f}")
    print(f"Test R²: {test_r2:.4f}")
    print(f"Test RMSE: {test_rmse:.4f}")
    print(f"Test MAE: {test_mae:.4f}")

# Model comparison
print("\n=== Model Comparison ===")
comparison_df = pd.DataFrame({
    name: {
        'Test R²': results['test_r2'],
        'Test RMSE': results['test_rmse'],
        'Test MAE': results['test_mae']
    }
    for name, results in model_results.items()
}).T

print(comparison_df.round(4))

# Choose best model for explanations (highest R²)
best_model_name = comparison_df['Test R²'].idxmax()
best_model = trained_models[best_model_name]
print(f"\nBest model for explanations: {best_model_name}")

# === Prediction vs Actual Analysis ===

# Plot predictions vs actual for all models
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, (name, results) in enumerate(model_results.items()):
    ax = axes[idx]
    y_pred = results['predictions']

    # Scatter plot
    ax.scatter(y_test, y_pred, alpha=0.6, s=20)

    # Perfect prediction line
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')

    ax.set_xlabel('Actual Engagement')
    ax.set_ylabel('Predicted Engagement')
    ax.set_title(f'{name}\nR² = {results["test_r2"]:.3f}')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Residual analysis for best model
best_predictions = model_results[best_model_name]['predictions']
residuals = y_test - best_predictions

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Residuals vs Predicted
axes[0].scatter(best_predictions, residuals, alpha=0.6)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('Predicted Engagement')
axes[0].set_ylabel('Residuals')
axes[0].set_title(f'{best_model_name}: Residuals vs Predicted')
axes[0].grid(True, alpha=0.3)

# Residuals histogram
axes[1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')
axes[1].set_xlabel('Residuals')
axes[1].set_ylabel('Frequency')
axes[1].set_title(f'{best_model_name}: Residuals Distribution')
axes[1].axvline(x=0, color='r', linestyle='--')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"Residuals statistics for {best_model_name}:")
print(f"Mean: {residuals.mean():.4f}")
print(f"Std: {residuals.std():.4f}")
print(f"MAE: {np.abs(residuals).mean():.4f}")



"""## Explaining the SHAP Plots

Here's a simple explanation of the SHAP plots generated above:

### SHAP Feature Importance (Bar Plot)

This plot shows which features were most important overall in predicting the product engagement score. The longer the bar, the more impact that feature had on the model's predictions, on average. The values on the x-axis represent the average absolute SHAP value for each feature.

### SHAP Summary Plot (Beeswarm Plot)

This plot gives a more detailed view of how each feature impacts the prediction.
- Each dot represents a single data point (a product).
- The position of the dot on the x-axis shows the SHAP value for that feature and data point. A positive SHAP value means the feature pushed the prediction higher, while a negative value pushed it lower.
- The color of the dot indicates the feature value for that data point (e.g., red for high values, blue for low values).
- Features are ordered by importance, with the most important at the top.

This plot helps us see:
- Which features have the biggest impact.
- Whether high or low values of a feature tend to increase or decrease the prediction.
- The distribution of the impact for each feature.

### SHAP Dependence Plots

These plots show how the SHAP value (impact on prediction) for a single feature changes as the value of that feature changes.
- Each dot is a data point.
- The x-axis is the value of the feature being plotted.
- The y-axis is the SHAP value for that feature for that data point.
- A horizontal line at y=0 indicates no impact.
- The plots can sometimes reveal interactions with another feature (represented by color), showing how the impact of the main feature changes depending on the value of the interacting feature.
"""

# === SHAP Global Explanations ===

# Use Random Forest for SHAP analysis (tree-based model works best with TreeExplainer)
rf_model = trained_models['Random Forest']
prep = rf_model.named_steps['prep']
rf_regressor = rf_model.named_steps['reg']

# Transform data to model input space
X_train_matrix = prep.transform(X_train)
X_test_matrix = prep.transform(X_test)

# Convert to dense numpy arrays if sparse and ensure proper dtype
if hasattr(X_train_matrix, 'toarray'):
    X_train_matrix = X_train_matrix.toarray()
if hasattr(X_test_matrix, 'toarray'):
    X_test_matrix = X_test_matrix.toarray()

# Ensure float64 dtype for SHAP compatibility
X_train_matrix = X_train_matrix.astype(np.float64)
X_test_matrix = X_test_matrix.astype(np.float64)

# Get feature names after preprocessing
ohe = prep.named_transformers_['cat'].named_steps['onehot']
num_feature_names = prep.transformers_[0][2]
cat_feature_names = ohe.get_feature_names_out(prep.transformers_[1][2])
bool_feature_names = prep.transformers_[2][2]
all_feature_names = list(num_feature_names) + list(cat_feature_names) + list(bool_feature_names)

print(f"Total features after preprocessing: {len(all_feature_names)}")
print(f"Numerical features: {len(num_feature_names)}")
print(f"Categorical features: {len(cat_feature_names)}")
print(f"Boolean features: {len(bool_feature_names)}")
print(f"Data shape for SHAP: {X_test_matrix.shape}")
print(f"Data dtype: {X_test_matrix.dtype}")

# Create SHAP explainer for regression
explainer = shap.TreeExplainer(rf_regressor)
shap_values = explainer.shap_values(X_test_matrix)

print(f"\nSHAP values shape: {shap_values.shape}")
print(f"Base value (model prediction average): {explainer.expected_value[0]:.4f}")

# Global feature importance
feature_importance = np.mean(np.abs(shap_values), axis=0)
importance_df = pd.DataFrame({
    'feature': all_feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print("\nTop 15 most important features:")
print(importance_df.head(15))

print(type(explainer.expected_value))
print(explainer.expected_value)

# === SHAP Visualizations ===

# Calculate feature importance for this cell
feature_importance = np.mean(np.abs(shap_values), axis=0)

# Summary plot (bar) - Global feature importance
plt.figure(figsize=(10, 8))
try:
    shap.summary_plot(shap_values, X_test_matrix, feature_names=all_feature_names, plot_type='bar', max_display=20, show=False)
    plt.title('SHAP Feature Importance - Global Impact on Engagement Prediction')
    plt.tight_layout()
    plt.show()
except Exception as e:
    print(f"Error creating SHAP bar plot: {e}")
    # Fallback: manual bar plot
    importance_df = pd.DataFrame({
        'feature': all_feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)

    top_features = importance_df.head(20)
    plt.figure(figsize=(10, 8))
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Mean |SHAP Value|')
    plt.title('SHAP Feature Importance - Global Impact on Engagement Prediction')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

# Summary plot (beeswarm) - Feature values and their impact
plt.figure(figsize=(10, 10))
try:
    shap.summary_plot(shap_values, X_test_matrix, feature_names=all_feature_names, max_display=20, show=False)
    plt.title('SHAP Summary Plot - Feature Values vs Impact on Engagement')
    plt.tight_layout()
    plt.show()
except Exception as e:
    print(f"Error creating SHAP summary plot: {e}")
    print("Skipping beeswarm plot due to compatibility issues")

# Dependence plots for top 4 features
print("\nCreating dependence plots for top 4 features:")
try:
    top_features_idx = np.argsort(feature_importance)[::-1][:4]

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.ravel()

    for i, idx in enumerate(top_features_idx):
        idx = int(idx)  # Ensure idx is Python integer
        feature_name = all_feature_names[idx]
        print(f"{i+1}. {feature_name}")

        try:
            # Use matplotlib for more control
            axes[i].scatter(X_test_matrix[:, idx], shap_values[:, idx], alpha=0.6, s=20)
            axes[i].set_xlabel(f'{feature_name} (Feature Value)')
            axes[i].set_ylabel('SHAP Value (Impact on Engagement)')
            axes[i].set_title(f'SHAP Dependence: {feature_name}')
            axes[i].grid(True, alpha=0.3)
            axes[i].axhline(y=0, color='red', linestyle='--', alpha=0.7)
        except Exception as e:
            print(f"Error creating dependence plot for {feature_name}: {e}")
            axes[i].text(0.5, 0.5, f'Error plotting\n{feature_name}',
                        transform=axes[i].transAxes, ha='center', va='center')

    plt.tight_layout()
    plt.show()

except Exception as e:
    print(f"Error creating dependence plots: {e}")

# Feature interaction analysis
print("\nAnalyzing feature interactions...")
try:
    interaction_strength = []
    for i in range(min(10, len(all_feature_names))):
        for j in range(i+1, min(10, len(all_feature_names))):
            # Simple interaction detection using correlation of SHAP values
            if shap_values.shape[1] > max(i, j):  # Ensure indices are valid
                interaction = np.corrcoef(shap_values[:, i], shap_values[:, j])[0, 1]
                if not np.isnan(interaction):  # Only include valid correlations
                    interaction_strength.append({
                        'feature_1': all_feature_names[i],
                        'feature_2': all_feature_names[j],
                        'interaction': abs(interaction)
                    })

    if interaction_strength:
        interaction_df = pd.DataFrame(interaction_strength).sort_values('interaction', ascending=False)
        print("\nTop 5 feature interactions (by SHAP correlation):")
        print(interaction_df.head())
    else:
        print("Could not compute feature interactions")

except Exception as e:
    print(f"Error in feature interaction analysis: {e}")

# === LIME Local Explanations ===

if LIME_AVAILABLE:
    print("=== LIME Local Explanations (Regression) ===")

    try:
        # Prepare data for LIME - ensure dense format
        X_train_matrix_dense = X_train_matrix.astype(np.float64)
        X_test_matrix_dense = X_test_matrix.astype(np.float64)

        # Create LIME explainer for regression
        lime_explainer = LimeTabularExplainer(
            training_data=X_train_matrix_dense,
            feature_names=all_feature_names,
            mode='regression',  # Regression mode!
            discretize_continuous=True
        )

        # Select diverse samples for explanation
        y_test_reset = y_test.reset_index(drop=True)  # Reset index for proper indexing
        sample_indices = [
            y_test_reset.idxmax(),  # Highest engagement
            y_test_reset.idxmin(),  # Lowest engagement
            len(y_test_reset)//2    # Medium engagement
        ]

        lime_explanations = []
        for i, idx in enumerate(sample_indices):
            try:
                # Get actual and predicted values
                actual_engagement = y_test_reset.iloc[idx]

                # Create single-row DataFrame for prediction
                X_single = X_test.iloc[[idx]]
                predicted_engagement = rf_model.predict(X_single)[0]

                print(f"\n--- Sample {i+1} (Index {idx}) ---")
                print(f"Actual engagement: {actual_engagement:.3f}")
                print(f"Predicted engagement: {predicted_engagement:.3f}")
                print(f"Prediction error: {abs(actual_engagement - predicted_engagement):.3f}")

                # Get LIME explanation
                instance = X_test_matrix_dense[idx]
                explanation = lime_explainer.explain_instance(
                    data_row=instance,
                    predict_fn=lambda x: rf_regressor.predict(x),
                    num_features=10
                )

                lime_explanations.append(explanation)

                # Print explanation
                print("\\nLIME explanation (feature impacts):")
                for feature, impact in explanation.as_list():
                    print(f"  {feature}: {impact:+.4f}")

                # Show in notebook (if running in Jupyter)
                try:
                    explanation.show_in_notebook(show_table=True)
                except:
                    pass  # Skip if not in Jupyter environment

            except Exception as e:
                print(f"Error explaining sample {i+1}: {e}")
                continue

    except Exception as e:
        print(f"Error setting up LIME: {e}")
        print("Skipping LIME analysis due to compatibility issues")

else:
    print("LIME not installed; skipping local explanation section.")
    print("Install with: pip install lime")

# === Regression Fairness Metrics ===

def regression_fairness_audit(y_true, y_pred, sensitive_feature, feature_name="sensitive_attribute"):
    """
    Comprehensive fairness audit for regression models.

    Metrics:
    - Mean prediction by group
    - RMSE by group
    - MAE by group
    - Prediction bias (mean prediction - mean actual)
    - R² by group
    """
    results = {}

    for group in sensitive_feature.unique():
        mask = sensitive_feature == group
        group_true = y_true[mask]
        group_pred = y_pred[mask]

        if len(group_true) > 0:
            rmse = np.sqrt(mean_squared_error(group_true, group_pred))
            mae = mean_absolute_error(group_true, group_pred)
            r2 = r2_score(group_true, group_pred)
            bias = group_pred.mean() - group_true.mean()

            results[group] = {
                'count': len(group_true),
                'mean_actual': group_true.mean(),
                'mean_predicted': group_pred.mean(),
                'rmse': rmse,
                'mae': mae,
                'r2': r2,
                'bias': bias,
                'std_actual': group_true.std(),
                'std_predicted': group_pred.std()
            }

    return results

def calculate_fairness_disparities(fairness_results):
    """
    Calculate disparities across groups.
    """
    metrics = ['mean_predicted', 'rmse', 'mae', 'r2', 'bias']
    disparities = {}

    for metric in metrics:
        values = [results[metric] for results in fairness_results.values()]
        disparities[metric] = {
            'max': max(values),
            'min': min(values),
            'range': max(values) - min(values),
            'std': np.std(values)
        }

    return disparities

# Conduct fairness audit
print("=== Regression Fairness Audit ===")

# Get predictions and sensitive feature for test set
rf_predictions = rf_model.predict(X_test)
speed_bucket_test = X_test['speed_bucket'].astype(str)

# Fairness analysis
fairness_results = regression_fairness_audit(
    y_true=y_test.values,
    y_pred=rf_predictions,
    sensitive_feature=speed_bucket_test,
    feature_name="speed_bucket"
)

# Display results
fairness_df = pd.DataFrame(fairness_results).T
print("\nFairness metrics by delivery speed group:")
print(fairness_df.round(4))

# Calculate disparities
disparities = calculate_fairness_disparities(fairness_results)
print("\nDisparities across groups:")
disparity_df = pd.DataFrame(disparities).T
print(disparity_df.round(4))

# Visual fairness analysis
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.ravel()

metrics_to_plot = ['mean_actual', 'mean_predicted', 'rmse', 'mae', 'r2', 'bias']
metric_labels = ['Mean Actual', 'Mean Predicted', 'RMSE', 'MAE', 'R²', 'Prediction Bias']

for i, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):
    groups = list(fairness_results.keys())
    values = [fairness_results[group][metric] for group in groups]

    bars = axes[i].bar(groups, values, alpha=0.7)
    axes[i].set_title(f'{label} by Delivery Speed')
    axes[i].set_ylabel(label)
    axes[i].grid(True, alpha=0.3)

    # Add value labels on bars
    for bar, value in zip(bars, values):
        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + abs(max(values))*0.01,
                    f'{value:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Statistical significance tests for group differences
from scipy import stats

print("\n=== Statistical Tests for Group Differences ===")
groups = list(fairness_results.keys())

if len(groups) >= 2:
    for i in range(len(groups)):
        for j in range(i+1, len(groups)):
            group1, group2 = groups[i], groups[j]

            # Get predictions for each group
            mask1 = speed_bucket_test == group1
            mask2 = speed_bucket_test == group2

            pred1 = rf_predictions[mask1]
            pred2 = rf_predictions[mask2]

            # T-test for difference in means
            t_stat, p_value = stats.ttest_ind(pred1, pred2)

            print(f"\n{group1} vs {group2}:")
            print(f"  Mean predictions: {pred1.mean():.3f} vs {pred2.mean():.3f}")
            print(f"  T-test p-value: {p_value:.6f}")
            print(f"  Significant difference (p<0.05): {'Yes' if p_value < 0.05 else 'No'}")

# === Bias Mitigation for Regression ===

def group_calibration_adjustment(y_true, y_pred, sensitive_feature):
    """
    Post-processing bias mitigation through group-specific calibration.
    Adjusts predictions to reduce group-wise bias.
    """
    adjusted_predictions = y_pred.copy()
    adjustments = {}

    for group in sensitive_feature.unique():
        mask = sensitive_feature == group
        if mask.sum() > 0:
            group_true = y_true[mask]
            group_pred = y_pred[mask]

            # Calculate group bias
            bias = group_pred.mean() - group_true.mean()

            # Adjust predictions by removing bias
            adjusted_predictions[mask] = group_pred - bias

            adjustments[group] = {
                'bias': bias,
                'adjustment': -bias,
                'count': mask.sum()
            }

    return adjusted_predictions, adjustments

def residual_reweighting(y_true, y_pred, sensitive_feature, alpha=0.5):
    """
    Bias mitigation through residual-based reweighting.
    Reduces predictions for over-predicted groups and increases for under-predicted groups.
    """
    adjusted_predictions = y_pred.copy()

    # Calculate overall residuals
    overall_residual = y_true.mean() - y_pred.mean()

    for group in sensitive_feature.unique():
        mask = sensitive_feature == group
        if mask.sum() > 0:
            group_true = y_true[mask]
            group_pred = y_pred[mask]

            # Calculate group-specific residual
            group_residual = group_true.mean() - group_pred.mean()

            # Apply partial correction (alpha controls strength)
            correction = alpha * group_residual
            adjusted_predictions[mask] = group_pred + correction

    return adjusted_predictions

print("=== Bias Mitigation Strategies ===")

# Original predictions analysis
print("\nOriginal model performance:")
original_fairness = regression_fairness_audit(y_test.values, rf_predictions, speed_bucket_test)
for group, metrics in original_fairness.items():
    print(f"{group}: bias = {metrics['bias']:.4f}, RMSE = {metrics['rmse']:.4f}")

# Method 1: Group calibration adjustment
print("\n--- Method 1: Group Calibration Adjustment ---")
adjusted_pred_1, adjustments = group_calibration_adjustment(
    y_test.values, rf_predictions, speed_bucket_test
)

print("Bias adjustments by group:")
for group, adj in adjustments.items():
    print(f"{group}: original bias = {adj['bias']:.4f}, adjustment = {adj['adjustment']:.4f}")

# Evaluate adjusted model
adjusted_fairness_1 = regression_fairness_audit(y_test.values, adjusted_pred_1, speed_bucket_test)
print("\nAfter calibration adjustment:")
for group, metrics in adjusted_fairness_1.items():
    print(f"{group}: bias = {metrics['bias']:.4f}, RMSE = {metrics['rmse']:.4f}")

# Method 2: Residual reweighting
print("\n--- Method 2: Residual Reweighting (α=0.7) ---")
adjusted_pred_2 = residual_reweighting(
    y_test.values, rf_predictions, speed_bucket_test, alpha=0.7
)

adjusted_fairness_2 = regression_fairness_audit(y_test.values, adjusted_pred_2, speed_bucket_test)
print("After residual reweighting:")
for group, metrics in adjusted_fairness_2.items():
    print(f"{group}: bias = {metrics['bias']:.4f}, RMSE = {metrics['rmse']:.4f}")

# Compare mitigation strategies
print("\n=== Mitigation Strategy Comparison ===")

strategies = {
    'Original': original_fairness,
    'Calibration': adjusted_fairness_1,
    'Reweighting': adjusted_fairness_2
}

# Calculate bias disparity (max bias - min bias across groups)
for strategy_name, fairness_result in strategies.items():
    biases = [metrics['bias'] for metrics in fairness_result.values()]
    bias_disparity = max(biases) - min(biases)
    rmse_values = [metrics['rmse'] for metrics in fairness_result.values()]
    avg_rmse = np.mean(rmse_values)

    print(f"{strategy_name}: Bias disparity = {bias_disparity:.4f}, Avg RMSE = {avg_rmse:.4f}")

# Visualize mitigation effects
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, (strategy_name, fairness_result) in enumerate(strategies.items()):
    groups = list(fairness_result.keys())
    biases = [fairness_result[group]['bias'] for group in groups]

    bars = axes[idx].bar(groups, biases, alpha=0.7)
    axes[idx].axhline(y=0, color='red', linestyle='--', alpha=0.7)
    axes[idx].set_title(f'{strategy_name}\nPrediction Bias by Group')
    axes[idx].set_ylabel('Prediction Bias')
    axes[idx].grid(True, alpha=0.3)

    # Add value labels
    for bar, bias in zip(bars, biases):
        axes[idx].text(bar.get_x() + bar.get_width()/2,
                      bar.get_height() + (0.01 if bias >= 0 else -0.01),
                      f'{bias:.3f}', ha='center',
                      va='bottom' if bias >= 0 else 'top')

plt.tight_layout()
plt.show()

"""## Conclusions & Discussion - Regression Version

### **Model Performance**
Our regression models successfully predicted continuous engagement scores with reasonable accuracy. The Random Forest achieved the best performance with R² scores indicating good predictive power while preserving all information in the continuous target variable.

### **Global Importance (SHAP)**
SHAP analysis revealed that **pricing signals, discount percentages, and social proof metrics (rating_count)** are the primary drivers of engagement scores. Unlike binary classification, we can now see the **magnitude** of impact each feature has on the actual engagement score, providing more nuanced business insights.

### **Local Explanations (LIME)**
LIME provided instance-level explanations showing how specific feature values contribute to individual engagement predictions. For high-engagement products, factors like optimal pricing and strong ratings consistently pushed scores upward, while delivery delays and low social proof reduced predicted engagement.

### **Fairness in Continuous Predictions**
Our custom regression fairness metrics revealed interesting patterns:
- **Prediction bias**: Some delivery speed groups were systematically over/under-predicted
- **Performance disparities**: RMSE and MAE varied across groups, indicating unequal prediction quality
- **Statistical significance**: T-tests confirmed that group differences were not due to random variation

### **Bias Mitigation Results**
Two mitigation strategies were tested:
1. **Group Calibration**: Directly removed group-wise bias, achieving near-zero bias but potentially affecting overall accuracy
2. **Residual Reweighting**: Partially corrected bias while maintaining prediction quality, offering a balanced trade-off

### **Advantages of Regression Approach**

**1. Information Preservation**
- Retains full granularity of engagement scores (e.g., 8.2 vs 8.7)
- No arbitrary threshold decisions
- Enables precise business targeting

**2. More Nuanced Insights**
- "Feature X increases engagement by 0.3 points" vs "Feature X increases probability of high engagement"
- Direct business interpretation of predicted scores
- Better understanding of feature magnitude effects

**3. Flexible Decision Making**
- Business can choose different thresholds for different decisions
- Supports risk-based decision making
- Enables optimization across continuous engagement spectrum

**4. Sophisticated Fairness Analysis**
- Multiple fairness dimensions (bias, accuracy, precision)
- Group-specific performance monitoring
- Granular bias correction strategies

### **Trade-offs and Considerations**

**Complexity vs Interpretability**
- Continuous predictions require more sophisticated interpretation
- Business stakeholders may prefer binary decisions
- Requires clear communication of prediction ranges and confidence

**Fairness Measurement**
- More complex fairness metrics than binary classification
- Multiple potential fairness definitions (bias, RMSE equality, etc.)
- Requires careful selection of appropriate fairness criteria

### **Business Applications**

**1. Product Portfolio Optimization**
- Identify products with predicted engagement scores below threshold
- Prioritize improvements based on predicted engagement gains
- Optimize pricing strategies using engagement-price relationships

**2. Inventory Management**
- Stock levels based on predicted engagement scores
- Dynamic pricing based on engagement predictions
- Promotional strategy optimization

**3. Fairness Monitoring**
- Continuous monitoring of prediction bias across customer segments
- Early detection of algorithmic discrimination
- Proactive bias correction in production systems

### **Recommendations**

**1. For Production Systems**
- Use regression for internal optimization and detailed analysis
- Provide both continuous scores and binary decisions for different stakeholders
- Implement real-time fairness monitoring

**2. For Business Reporting**
- Create engagement score bands (e.g., 0-3: Low, 3-7: Medium, 7-10: High)
- Show both predicted scores and confidence intervals
- Provide actionable recommendations based on score ranges

**3. For Continuous Improvement**
- Regular fairness audits across different customer segments
- A/B testing of bias mitigation strategies
- Continuous model retraining with fairness constraints

### **Future Enhancements**

1. **Advanced Bias Mitigation**: Implement in-processing fairness constraints during model training
2. **Uncertainty Quantification**: Add prediction intervals to communicate model confidence
3. **Dynamic Thresholding**: Automatically adjust decision thresholds based on business objectives
4. **Multi-objective Optimization**: Balance accuracy, fairness, and business KPIs simultaneously
5. **Causal Analysis**: Move beyond correlation to understand causal relationships in engagement

### **Key Takeaway**

The regression approach provides **richer, more actionable insights** while maintaining explainability and enabling sophisticated fairness analysis. By preserving the continuous nature of engagement scores, we enable more nuanced business decisions and better understanding of feature impacts on customer behavior.

This approach is particularly valuable for e-commerce applications where understanding the **degree** of engagement (not just high vs low) is crucial for optimization strategies, pricing decisions, and fair treatment of different customer segments.
"""