# -*- coding: utf-8 -*-
"""Copy of DS_EXP_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17WMWi8QZqUcK3YsOsu6H5_xwZ3NorzvD

# Experiment 3
"""

# Data manipulation and analysis
import pandas as pd
import numpy as np

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Statistical analysis
from scipy import stats
from scipy.stats import normaltest, shapiro, kstest, ttest_ind, chi2_contingency
import statsmodels.api as sm
from statsmodels.stats.diagnostic import lilliefors

# Distribution fitting
from scipy.stats import norm, poisson, expon, gamma

# Warnings
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("‚úÖ All libraries imported successfully!")

# Load the cleaned dataset
df = pd.read_csv("/content/Zudio_Scrapped_Final_Data.csv")

print("üìä Dataset Overview:")
print(f"Shape: {df.shape}")
print(f"Columns: {list(df.columns)}")
print("\nüìà First 5 rows:")
display(df.head())

print("\nüìã Data Types:")
print(df.dtypes)

print("\nüîç Basic Statistics:")
display(df.describe())

"""## Step 1: Plot Class Balance
We start by examining the **class distribution** of target variables (e.g., `trending`, `is_bestseller`, or category fields).  
This helps us understand if the dataset is **balanced** or if it suffers from **class imbalance**, which can affect model training.

"""

# Create subplots for class balance analysis
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Class Balance Analysis', fontsize=16, fontweight='bold')

# 1. Category distribution
category_counts = df['category'].value_counts()
axes[0, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')
axes[0, 0].set_title('Category Distribution (Pie Chart)')

# 2. Category count plot
sns.countplot(data=df, y='category', ax=axes[0, 1], order=df['category'].value_counts().index)
axes[0, 1].set_title('Category Distribution (Count Plot)')
axes[0, 1].set_xlabel('Count')

# 3. Availability distribution
if 'availability' in df.columns:
    sns.countplot(data=df, x='availability', ax=axes[1, 0])
    axes[1, 0].set_title('Availability Distribution')
    axes[1, 0].tick_params(axis='x', rotation=45)

# 4. Brand distribution (top 10)
if 'brand' in df.columns:
    top_brands = df['brand'].value_counts().head(10)
    sns.barplot(x=top_brands.values, y=top_brands.index, ax=axes[1, 1])
    axes[1, 1].set_title('Top 10 Brands Distribution')
    axes[1, 1].set_xlabel('Count')

plt.tight_layout()
plt.show()

# Print class balance statistics
print("üìä Class Balance Statistics:")
print(f"\nüè∑Ô∏è Category Distribution:")
print(df['category'].value_counts())
print(f"\nüìà Category Proportions:")
print(df['category'].value_counts(normalize=True).round(3))

"""## Step 2: Visualize Feature Distributions
We now visualize **continuous features** (e.g., price, rating, rating_count) using:
- **Histograms** ‚Üí to check skewness and modality.
- **Boxplots** ‚Üí to detect presence of **outliers**.

"""

# Identify numerical columns
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
print(f"üìä Numerical columns: {numerical_cols}")

# Create distribution plots for numerical features
n_cols = len(numerical_cols)
n_rows = (n_cols + 2) // 3  # 3 columns per row

fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6*n_rows))
fig.suptitle('Distribution of Numerical Features', fontsize=16, fontweight='bold')

axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes

for i, col in enumerate(numerical_cols[:len(axes)]):
    # Histogram with KDE
    sns.histplot(data=df, x=col, kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of {col}')
    axes[i].axvline(df[col].mean(), color='red', linestyle='--', label=f'Mean: {df[col].mean():.2f}')
    axes[i].axvline(df[col].median(), color='green', linestyle='--', label=f'Median: {df[col].median():.2f}')
    axes[i].legend()

# Hide empty subplots
for i in range(len(numerical_cols), len(axes)):
    axes[i].set_visible(False)

plt.tight_layout()
plt.show()

import numpy as np

# set tunable parameters
alpha = 2.5   # weight for trending
beta = 4.5    # weight for bestseller

# compute core term
core_term = df['rating'] * np.log1p(df['rating_count'])

# final product_engagement formula
df['product_engagement'] = (
    core_term
    + alpha * df['trending'].astype(int)
    + beta * df['is_bestseller'].astype(int)
)

# preview
print(df[['rating', 'rating_count', 'trending', 'is_bestseller', 'product_engagement']].head())

print(df[['rating', 'rating_count', 'trending', 'is_bestseller', 'product_engagement']].head())

"""## Step 3: Assess Feature Correlations
We create a **correlation matrix heatmap** to understand linear relationships between numerical features.  
This helps in:
- Identifying **multicollinearity** between predictors.
- Discovering strongly related features useful for prediction.

"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Step 1: Add product_engagement to your dataframe (if not already added)
alpha, beta = 2.5, 4.5
# df['product_engagement'] = (
#     df['rating'] * np.log1p(df['rating_count'])
#     + alpha * df['trending'].astype(int)
#     + beta * df['is_bestseller'].astype(int)
# )

# Step 2: Select numerical columns (including new column)
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Step 3: Calculate correlation matrix
correlation_matrix = df[numerical_cols].corr()

# Step 4: Create correlation heatmap
plt.figure(figsize=(12, 10))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle
sns.heatmap(correlation_matrix,
            mask=mask,
            annot=True,
            cmap='RdBu_r',
            center=0,
            square=True,
            fmt='.2f',
            cbar_kws={"shrink": .8})
plt.title('Correlation Matrix Heatmap (Including Product Engagement)', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# Step 5: Find strong correlations (absolute value > 0.7)
strong_correlations = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_value = correlation_matrix.iloc[i, j]
        if abs(corr_value) > 0.6:
            strong_correlations.append((correlation_matrix.columns[i],
                                        correlation_matrix.columns[j],
                                        corr_value))

print("üîó Strong Correlations (|r| > 0.6):")
if strong_correlations:
    for feature1, feature2, corr in strong_correlations:
        print(f"  {feature1} ‚Üî {feature2}: {corr:.3f}")
else:
    print("  No strong correlations found.")

!pip install nbformat

# Interactive correlation heatmap using Plotly
fig = go.Figure(data=go.Heatmap(
    z=correlation_matrix.values,
    x=correlation_matrix.columns,
    y=correlation_matrix.columns,
    colorscale='RdBu',
    zmid=0,
    text=correlation_matrix.round(2).values,
    texttemplate="%{text}",
    textfont={"size": 10},
    hoverongaps=False
))

fig.update_layout(
    title='Interactive Correlation Matrix',
    xaxis_nticks=36,
    width=800,
    height=600
)

fig.show()

"""## Step 4: Feature Distribution Analysis & Outlier Detection
We fit statistical distributions (e.g., **Binomial**) to numerical features and
check how well the data follows these theoretical distributions.  
This helps in identifying **outliers** and **statistical properties**.

## Explanation of Chosen Distributions

Based on the nature of the variables and common statistical practices, the following distributions were chosen for analysis:

- **Continuous Features (price, product_engagement):** For continuous data, we typically explore distributions like **Normal**, **Exponential**, **Gamma**, and **Log-Normal**. These are common choices for modeling positive-valued continuous variables like prices or calculated engagement scores, which can often be skewed. The AIC (Akaike Information Criterion) is used to compare the goodness of fit among these distributions, with lower AIC values indicating a better fit.

- **Discrete Features (rating):** For discrete data like ratings (which are often integer values or can be treated as such), the **Poisson distribution** is a common starting point for modeling count data or events occurring at a certain rate. While ratings might not perfectly fit a Poisson distribution (as they are bounded and not strictly counts of events), it serves as a reasonable theoretical distribution to compare against the observed distribution.

- **Binary Features (is_bestseller, trending):** For binary variables with only two possible outcomes (True/False or 1/0), the **Bernoulli distribution** is the fundamental model for a single trial, and the **Binomial distribution** models the number of successes in a fixed number of independent Bernoulli trials. These distributions are appropriate for analyzing the proportion of products that are bestsellers or trending.
"""

categorical_cols = df[['is_bestseller', 'trending']]
categorical_cols = categorical_cols.astype(int)
print(categorical_cols)
# Step 3: Calculate correlation matrix
correlation_matrix = df[categorical_cols].corr()

# Step 4: Create correlation heatmap
plt.figure(figsize=(12, 10))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle
sns.heatmap(correlation_matrix,
            mask=mask,
            annot=True,
            cmap='RdBu_r',
            center=0,
            square=True,
            fmt='.2f',
            cbar_kws={"shrink": .8})
plt.title('Correlation Matrix Heatmap (Including Product Engagement)', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# Step 5: Find strong correlations (absolute value > 0.7)
strong_correlations = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_value = correlation_matrix.iloc[i, j]
        if abs(corr_value) > 0.6:
            strong_correlations.append((correlation_matrix.columns[i],
                                        correlation_matrix.columns[j],
                                        corr_value))

print("üîó Strong Correlations (|r| > 0.6):")
if strong_correlations:
    for feature1, feature2, corr in strong_correlations:
        print(f"  {feature1} ‚Üî {feature2}: {corr:.3f}")
else:
    print("  No strong correlations found.")

import matplotlib.pyplot as plt
from scipy.stats import bernoulli, binom

binary_cols = ['trending', 'is_bestseller']

for col in binary_cols:
    if col in df.columns:
        print(f"\nüìä Analysis for {col}")
        print("="*40)

        # Data
        values = df[col].dropna()
        n = len(values)
        p_hat = values.mean()  # estimated probability of success

        print(f"  Total products: {n}")
        print(f"  Successes (1s): {values.sum()}")
        print(f"  Failures (0s): {n - values.sum()}")
        print(f"  Estimated p: {p_hat:.3f}")

        # --- Bernoulli Distribution (per product) ---
        print("\nüîπ Bernoulli Distribution (per product)")
        print(f"  P(X=1) = {p_hat:.3f}, P(X=0) = {1-p_hat:.3f}")

        # --- Binomial Distribution (across all products) ---
        print("\nüîπ Binomial Distribution (across products)")
        k = range(0, n+1)
        binom_pmf = binom.pmf(k, n, p_hat)

        print(f"  Parameters: n={n}, p={p_hat:.3f}")
        print(f"  Expected successes: {n*p_hat:.2f}")
        print(f"  Variance: {n*p_hat*(1-p_hat):.2f}")

        # Visualization
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))

        # Bernoulli visualization
        bernoulli_pmf = [1-p_hat, p_hat]
        axes[0].bar([0,1], bernoulli_pmf, color='skyblue')
        axes[0].set_xticks([0,1])
        axes[0].set_title(f'Bernoulli({p_hat:.2f}) for {col}')
        axes[0].set_ylabel('Probability')

        # Binomial visualization
        axes[1].bar(k, binom_pmf, color='lightgreen')
        axes[1].set_title(f'Binomial(n={n}, p={p_hat:.2f}) for {col}')
        axes[1].set_xlabel('Number of successes')
        axes[1].set_ylabel('Probability')

        plt.tight_layout()
        plt.show()

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import poisson

print("üìà Distribution Fitting Analysis for Fashion Dataset")
print("="*50)

# Continuous variables to test distributions on
continuous_features = []
for col in ['price', 'product_engagement']:
    if col in df.columns:
        continuous_features.append(col)

# Candidate continuous distributions
distributions = {
    'Normal': stats.norm,
    'Exponential': stats.expon,
    'Gamma': stats.gamma,
    'Log-Normal': stats.lognorm
}

# -------------------------------
# 1. Continuous Features
# -------------------------------
for feature in continuous_features:
    values = df[feature].dropna()
    print(f"\nüîç Fitting Distributions for {feature} (n={len(values)})")

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.flatten()

    aic_scores = {}

    for i, (dist_name, distribution) in enumerate(distributions.items()):
        # Fit distribution
        if dist_name == 'Exponential':
            params = distribution.fit(values, floc=0)  # Exponential usually fixed at 0
        else:
            params = distribution.fit(values)

        x = np.linspace(values.min(), values.max(), 100)
        y = distribution.pdf(x, *params)

        # Calculate AIC
        log_likelihood = np.sum(distribution.logpdf(values, *params))
        aic = 2 * len(params) - 2 * log_likelihood
        aic_scores[dist_name] = aic

        # Plot histogram with fitted PDF
        axes[i].hist(values, bins=30, density=True, alpha=0.7, label='Data')
        axes[i].plot(x, y, 'r-', label=f'{dist_name} fit')
        axes[i].set_title(f'{dist_name} Distribution Fit\nAIC: {aic:.2f}')
        axes[i].set_xlabel(feature)
        axes[i].set_ylabel('Density')
        axes[i].legend()

        print(f"\n{dist_name} Distribution:")
        print(f"  Parameters: {params}")
        print(f"  AIC Score: {aic:.2f}")

    plt.tight_layout()
    plt.show()

    # Best fitting distribution
    best_dist = min(aic_scores, key=aic_scores.get)
    print(f"üèÜ Best Fitting Distribution for {feature}: {best_dist} (Lowest AIC: {aic_scores[best_dist]:.2f})")

# -------------------------------
# 2. Discrete Feature (Rating)
# -------------------------------
rating_col = None
for col in ['rating', 'Rating', 'stars', 'score']:
    if col in df.columns:
        rating_col = col
        break

if rating_col:
    ratings = df[rating_col].dropna().astype(int)

    print(f"\nüìä Discrete Distribution Analysis for {rating_col}:")

    # Count frequency
    rating_counts = ratings.value_counts().sort_index()

    # Fit Poisson distribution
    lambda_param = ratings.mean()
    poisson_probs = [poisson.pmf(k, lambda_param) for k in rating_counts.index]

    # Plot comparison
    plt.figure(figsize=(10, 6))
    x_pos = np.arange(len(rating_counts))

    plt.bar(x_pos - 0.2, rating_counts.values / rating_counts.sum(),
            width=0.4, label='Observed', alpha=0.7)
    plt.bar(x_pos + 0.2, poisson_probs,
            width=0.4, label=f'Poisson (Œª={lambda_param:.2f})', alpha=0.7)

    plt.xlabel(rating_col)
    plt.ylabel('Probability')
    plt.title(f'{rating_col} Distribution vs Poisson Fit')
    plt.xticks(x_pos, rating_counts.index)
    plt.legend()
    plt.show()

    print(f"  Mean {rating_col}: {ratings.mean():.2f}")
    print(f"  Poisson parameter (Œª): {lambda_param:.2f}")



"""## Step 5: Hypothesis Testing (t-test)
We test whether the **mean values** of continuous variables differ significantly between groups
(e.g., trending vs non-trending products).  
- **H0 (Null Hypothesis):** The means are equal.  
- **H1 (Alternative Hypothesis):** The means are significantly different.

**X: is_bestseller**
**Y: is_trending**

  As both the variable are categorical we are choosing Chi-square Test of independence to find relation between the two variables X & Y

"""

# Perform Chi-square test of independence between 'is_bestseller' and 'trending'
contingency_table = pd.crosstab(df['is_bestseller'], df['trending'])

print("Contingency Table:")
print(contingency_table)

chi2, p, dof, expected = chi2_contingency(contingency_table)

print(f"\nChi-square statistic: {chi2:.4f}")
print(f"P-value: {p:.4f}")
print(f"Degrees of freedom: {dof}")
print("Expected frequencies:")
print(expected)

# Interpret the results
alpha = 0.05
print(f"\nSignificance level (alpha): {alpha}")
if p < alpha:
    print("Result: Reject the null hypothesis. There is a significant association between 'is_bestseller' and 'trending'.")
else:
    print("Result: Fail to reject the null hypothesis. There is no significant association between 'is_bestseller' and 'trending'.")