# -*- coding: utf-8 -*-
"""DS_EXP4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rxBDpRh2X310ClxMIgIYWa1gEtvQziag
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/SMA && DS SEM_7üë©‚Äçüéìüë®‚Äçüéìüë®‚Äçüéìüë®‚Äçüíªüë®‚Äçüíªüë©‚Äçüíª/DS/processed_zudio_data.csv")
print(df.head())
print("\nColumn names:")
print(df.columns.tolist())
print("\nColumn types:")
print(df.dtypes)

# Define target column to predict
target_col = 'product_engagement'
# Features
X = df.drop(target_col, axis=1)
y = df[target_col]

# Automatically detect column types
numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
boolean_cols = X.select_dtypes(include=['bool']).columns.tolist()

print("Numeric columns:", numeric_cols)
print("Categorical columns:", categorical_cols)
print("Boolean columns:", boolean_cols)

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols + boolean_cols)
    ]
)

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# Define models
models = {
    'LinearRegression': LinearRegression(),
    'RandomForest': RandomForestRegressor(random_state=42),
    'GradientBoosting': GradientBoostingRegressor(random_state=42)
}

baseline_results = {}
pipelines = {}

for name, model in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)  # Calculate MSE
    rmse = np.sqrt(mse)  # Calculate RMSE
    baseline_results[name] = rmse
    pipelines[name] = pipeline
    print(f"{name} RMSE: {rmse:.4f}")

from sklearn.model_selection import GridSearchCV

# Hyperparameter grids
param_grids = {
    'RandomForest': {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__max_depth': [None, 10, 20],
        'regressor__min_samples_split': [2, 5]
    },
    'GradientBoosting': {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__learning_rate': [0.01, 0.05, 0.1],
        'regressor__max_depth': [3, 5, 7]
    },
    'LinearRegression': {
        # LinearRegression has no hyperparameters to tune in sklearn; we will skip tuning
    }
}

tuned_results = {}
tuned_pipelines = {}

for name, model in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    if param_grids.get(name):
        grid_search = GridSearchCV(pipeline, param_grids[name], cv=3, n_jobs=-1)
        grid_search.fit(X_train, y_train)
        best_pipeline = grid_search.best_estimator_
        y_pred = best_pipeline.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        tuned_results[name] = rmse
        tuned_pipelines[name] = best_pipeline
        print(f"{name} Tuned RMSE: {rmse:.4f} | Best Params: {grid_search.best_params_}")
    else:
        # No tuning for LinearRegression, just use baseline
        tuned_results[name] = baseline_results[name]
        tuned_pipelines[name] = pipelines[name]
        print(f"{name} RMSE (no tuning): {baseline_results[name]:.4f}")

!pip install mlflow

!pip install lightgbm

from lightgbm import LGBMRegressor

# Add LightGBM to models
models['LightGBM'] = LGBMRegressor(random_state=42)

# Add LightGBM hyperparameters to param_grids
param_grids['LightGBM'] = {
    'regressor__n_estimators': [50, 100, 200],
    'regressor__learning_rate': [0.01, 0.05, 0.1],
    'regressor__num_leaves': [31, 63, 127]
}

tuned_results = {}
tuned_pipelines = {}

for name, model in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    if param_grids.get(name):
        grid_search = GridSearchCV(pipeline, param_grids[name], cv=3, n_jobs=-1)
        grid_search.fit(X_train, y_train)
        best_pipeline = grid_search.best_estimator_
        y_pred = best_pipeline.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        tuned_results[name] = rmse
        tuned_pipelines[name] = best_pipeline
        print(f"{name} Tuned RMSE: {rmse:.4f} | Best Params: {grid_search.best_params_}")
    else:
        # No tuning for LinearRegression, just use baseline
        tuned_results[name] = baseline_results[name]
        tuned_pipelines[name] = pipelines[name]
        print(f"{name} RMSE (no tuning): {baseline_results[name]:.4f}")

import joblib
import mlflow
import mlflow.sklearn

# Select best model
best_model_name = min(tuned_results, key=tuned_results.get)
best_pipeline = tuned_pipelines[best_model_name]
best_rmse = tuned_results[best_model_name]

# Save model as .pkl
joblib.dump(best_pipeline, "best_model.pkl")
print(f"Best model '{best_model_name}' saved with RMSE: {best_rmse:.4f}")

# MLflow logging
with mlflow.start_run():
    mlflow.log_param("best_model", best_model_name)
    mlflow.log_metric("RMSE", best_rmse)
    mlflow.sklearn.log_model(best_pipeline, "best_model")
    print("MLflow logging completed.")

import pandas as pd

# Create DataFrame for comparison
comparison = pd.DataFrame({
    'Model': ['RandomForest', 'LinearRegression', 'GradientBoosting'],
    'Baseline_RMSE': [baseline_results['RandomForest'],
                       baseline_results['LinearRegression'],
                       baseline_results['GradientBoosting']],
    'Tuned_RMSE': [tuned_results['RandomForest'],
                    tuned_results['LinearRegression'],
                    tuned_results['GradientBoosting']]
})

# Calculate improvement
comparison['Improvement'] = comparison['Baseline_RMSE'] - comparison['Tuned_RMSE']

print("Comparative Analysis of Baseline vs Tuned Models:")
print(comparison)

"""##  **Why These 3 Models Were Chosen**

### **1. RandomForest - The Robust Generalist**
#### Why RandomForest for product_engagement?
- Handles mixed data types well (numeric + categorical features)
- Resistant to overfitting
- Provides feature importance rankings
- Works well with e-commerce data patterns
- No need for feature scaling (built-in)

**Perfect for your case because:**
- **E-commerce data complexity**: Your dataset has mixed features (price, category, ratings, etc.)
- **Non-linear relationships**: Product engagement often has complex patterns
- **Feature interaction**: Can capture how category + price + rating work together

### **2. LinearRegression - The Interpretable Baseline**
#### Why LinearRegression as baseline?
- Fast training and prediction
- Highly interpretable coefficients  
- Good baseline to beat
- Assumes linear relationship between features and engagement

**Strategic choice because:**
- **Business interpretability**: "For every $1 price increase, engagement decreases by X"
- **Baseline performance**: If complex models don't beat this, stick with simple
- **Fast experimentation**: Quick to test feature combinations

### **3. GradientBoosting - The Performance Champion**
##### Why GradientBoosting for engagement prediction?
- Excellent predictive performance
- Handles feature interactions automatically
- Sequential learning (learns from previous mistakes)
- Good with tabular data

**Ideal for your use case:**
- **High accuracy needs**: Engagement prediction needs precision
- **Complex patterns**: Can capture subtle engagement drivers
- **Robust to outliers**: Important for e-commerce data

##  **Why Other Models Weren't Used**

### **Deep Learning Models (Neural Networks)**
#### Why NOT used?
- Overkill for tabular data with ~10-20 features
- Requires much more data (you likely have <100k samples)
- Black box (less interpretable for business)
- Computationally expensive for this problem size

### **SVM (Support Vector Machines)**
#### Why NOT used?
- Doesn't handle categorical features well natively
- Slow on medium-large datasets
- Less interpretable than tree models
- Not ideal for regression on mixed data types

### **KNN (K-Nearest Neighbors)**
#### Why NOT used?
- Poor performance with high-dimensional data
- Sensitive to irrelevant features
- Computationally expensive for prediction
- Not great with mixed data types

### **Naive Bayes**
#### Why NOT used?
- Designed for classification, not regression
- Strong independence assumption (unrealistic for engagement)
- Poor with continuous features

## **The Strategic Thinking Behind This Choice**

### **Coverage Strategy**

Linear Model     ‚Üí LinearRegression    (simple, interpretable)

Tree-based       ‚Üí RandomForest        (robust, feature importance)  

Boosting         ‚Üí GradientBoosting    (high performance)

### **Business Requirements**
Interpretability ‚Üí LinearRegression coefficients

Robustness      ‚Üí RandomForest handles messy data

Performance     ‚Üí GradientBoosting for best predictions

## **What Could Have Been Added**

### **For Even Better Results:**
"""

# Advanced models that could improve performance:
models_advanced = {
    'XGBoost': XGBRegressor(),           # Often best for tabular data
    'LightGBM': LGBMRegressor(),         # Faster than XGBoost
    'CatBoost': CatBoostRegressor(),     # Handles categories natively
    'ElasticNet': ElasticNet(),          # Regularized linear model
}

"""### **For Specific Use Cases:**
#### If you had more data/features:
'Neural Network': MLPRegressor()        # For complex patterns

'Ensemble': VotingRegressor()           # Combine multiple models

'Ridge/Lasso': Ridge(), Lasso()         # Feature selection

## **The Bottom Line**

We chose these 3 models because they:

1. **Cover the spectrum**: Simple ‚Üí Moderate ‚Üí Complex
2. **Handle your data type**: Mixed numeric/categorical features  
3. **Business-appropriate**: Balance of performance and interpretability
4. **Proven for e-commerce**: These models work well for engagement prediction
5. **Resource-efficient**: Don't need massive compute or data

For a **product engagement prediction** problem with mixed data types and moderate dataset size, this is actually a **very smart model selection**!
"""

!mlflow ui

# MLflow: log best pipeline with metrics (fixed)
import mlflow
import mlflow.sklearn
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Assumes best_pipeline, best_model_name, X_train, y_train, X_test, y_test exist from previous cells
with mlflow.start_run(run_name=f"best_model_{best_model_name}"):
    # Params
    mlflow.log_param("best_model", best_model_name)

    # Train metrics
    y_pred_train = best_pipeline.predict(X_train)
    train_rmse = float(np.sqrt(mean_squared_error(y_train, y_pred_train)))
    train_mae = float(mean_absolute_error(y_train, y_pred_train))
    train_r2 = float(r2_score(y_train, y_pred_train))

    # Test metrics
    y_pred_test = best_pipeline.predict(X_test)
    test_rmse = float(np.sqrt(mean_squared_error(y_test, y_pred_test)))
    test_mae = float(mean_absolute_error(y_test, y_pred_test))
    test_r2 = float(r2_score(y_test, y_pred_test))

    # Log metrics
    mlflow.log_metrics({
        "train_rmse": train_rmse,
        "train_mae": train_mae,
        "train_r2": train_r2,
        "test_rmse": test_rmse,
        "test_mae": test_mae,
        "test_r2": test_r2,
    })

    # Log model artifact with a small input example
    mlflow.sklearn.log_model(
        sk_model=best_pipeline,
        artifact_path="model",
        input_example=X_train.head(5)
    )

print("MLflow logging completed. Test metrics:", {"rmse": test_rmse, "mae": test_mae, "r2": test_r2})

!zip -r mlruns.zip mlruns/